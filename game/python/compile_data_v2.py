import os
from pydoc import text
import re
import json
import yaml
import argparse
import sys
import shlex
import hashlib
from pathlib import Path
from .helpers import (parse_markdown)

from .compile_helpers import (
    parse_markdown,
    parse_yaml_list,
    parse_yaml_block,
    parse_counts,
    parse_csv,
    parse_bool,
    parse_int,
    parse_float,
    parse_kv_block,
    slugify_segment,
    label_safe,
    _lines_to_text,
    extract_locations_section,
    extract_named_section,
    find_named_section,
    split_sections,
    extract_flow_blocks,
    extract_yaml_block,
)

types = {
    "item": {},
    "character": {},
    "location": {},
    "quest": {},
    "container": {},
    "stat": {},
    "faction": {},
    "slot": {},
    "body_type": {},
    "dialogue": {},
    "story_origin": {},
    "shop": {},
    "craft": {},
    "note": {},
    "achievement": {},
    "perk": {},
    "status_effect": {},
    "bond": {}
}

def compile_data():
    all_ids = {} # To test for collisions.
    data_consolidated = {k: {} for k in types.keys()}

    base_dir = Path(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
    data_dir = base_dir / "data"

    for md_file in data_dir.rglob("*.md"):
       md_data = md_file.read_text(encoding="utf-8")
       fm, heads = parse_markdown(md_data)
       if "type" in fm:
              obj_type = fm["type"]
              obj_id = fm.get("id") or fm.get("name").lower().replace(" ", "_")
              if obj_type in types and obj_id:
                if obj_id in all_ids:
                     print(f"Warning: Duplicate ID '{obj_id}' found in {md_file} (also in {all_ids[obj_id]})")
                all_ids[obj_id] = obj_type
                data_consolidated[obj_type][obj_id] = fm
    
    rpy_script = []
    rpy_script.append("# This file is auto-generated by compile_data_v2.py. Do not edit manually.")
    for item_id, item_data in data_consolidated["item"].items():
        kwarg_str = ", ".join(f"{k}={repr(v)}" for k, v in item_data.items() if k not in ["id", "type"])
        rpy_script.append(f'define {item_id} = Item("{item_id}", {kwarg_str})')
    rpy_script.append("define all_items = [{", ", ".join(f"{item_id}" for item_id in data_consolidated["item"].keys()), "}]")
    rpy_output_file = base_dir / "game" / "generated" / "data_definitions.rpy"
    with rpy_output_file.open("w", encoding="utf-8") as f:
        f.write("\n".join(rpy_script))
    print(f"RPY definitions written to {rpy_output_file}")

    # game_dir = base_dir / "game" / "generated"
    # game_dir.mkdir(exist_ok=True)
    # output_file = game_dir / "compiled_data_v2.json"
    # with output_file.open("w", encoding="utf-8") as f:
    #     json.dump(data_consolidated, f, ensure_ascii=False, indent=2)
    # print(f"Compiled data written to {output_file}")